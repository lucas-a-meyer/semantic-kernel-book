{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A whirlwind tour through the Microsoft Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.planning.basic_planner import BasicPlanner\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_ENDPOINT = os.getenv(\"OPENAI_USEAST3_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_USEAST3_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Kernel\n",
    "\n",
    "A cool thing about Semantic Kernel is that it supports multiple models. This enables you to run simple workloads on cheaper models, and expensive workloads on expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x1ce6cf6ead0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "gpt35 = AzureChatCompletion(deployment_name=\"gpt-35-turbo\", # yours may be different\n",
    "                            endpoint=OPENAI_ENDPOINT,\n",
    "                            api_key=OPENAI_API_KEY)\n",
    "\n",
    "gpt4 = AzureChatCompletion(deployment_name=\"gpt-4\", # yours may be different\n",
    "                            endpoint=OPENAI_ENDPOINT,\n",
    "                            api_key=OPENAI_API_KEY)\n",
    "\n",
    "kernel.add_chat_service(\"gpt35\", gpt35)\n",
    "kernel.add_chat_service(\"gpt4\", gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the chat service\n",
    "\n",
    "One of the simplest things you can do is simply execute a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes the police, open up!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"knock, knock? Who’s there? {{$input}}. {{$input}} who?\"\"\"\n",
    "knock = kernel.create_semantic_function(prompt, temperature=0.8)\n",
    "response = knock(\"Dishes\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic functions\n",
    "\n",
    "A **semantic function** is a function that interacts with a large language model (LLM).\n",
    "\n",
    "Although you can define a semantic function with a `dict` for the configuration and a `string` for the prompt as we did above, in production, we usually prefer to separate the code from the prompts and configuration. \n",
    "\n",
    "### Plugins\n",
    "\n",
    "Collections of semantic functions are called **Plugins**. Plugins are simply folders that contain semantic functions. Each semantic function should be in a separate folder.\n",
    "\n",
    "Each semantic function is defined by two files : `skprompt.txt` that contains the prompt (including placeholders for parameters) and `config.json`, that contains the configuration, such as default *temperature*, default *service*, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross_the_road_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1ce6e6520d0>,\n",
       " 'genie_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1ce7d72aa10>,\n",
       " 'knock_knock_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1ce7d72b390>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_plugin = kernel.import_semantic_skill_from_directory(\"plugins\", \"jokes\")\n",
    "jokes_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a function from a plugin\n",
    "\n",
    "Once you loaded the functions into the Kernel, you can load them into a varaible or simply accessing them directly from the plugin object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knock, knock?\n",
      "Who's there?\n",
      "Dishes.\n",
      "Dishes who?\n",
      "Dishes your friend, open the door and let me in!\n"
     ]
    }
   ],
   "source": [
    "response = jokes_plugin[\"knock_knock_joke\"](\"dishes\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the function into a function variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the atom cross the road?\n",
      "\n",
      "Because it wanted to split!\n"
     ]
    }
   ],
   "source": [
    "cross_the_road = jokes_plugin[\"cross_the_road_joke\"]\n",
    "response = cross_the_road(\"atom\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `genie_joke` function has multiple parameters. To pass them, you have to create an object of the `ContextVariables` class, and pass it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three men were on a deserted island and found a genie. The genie gave each one wish. \n",
      "\n",
      "The first said he wished to go home and instantly transformed into a homing pigeon.\n",
      "\n",
      "The second said he wished to go home and in a puff of smoke, he turned into a houseplant in his living room.\n",
      "\n",
      "The third person’s wish was to have both his friends back on the island because he was getting bored.\n"
     ]
    }
   ],
   "source": [
    "context_variables = sk.ContextVariables()\n",
    "context_variables[\"firstWish\"] = \"go home\"\n",
    "context_variables[\"secondWish\"] = \"go home\"\n",
    "\n",
    "response = jokes_plugin[\"genie_joke\"](variables=context_variables)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native functions\n",
    "\n",
    "Native functions are pure Python code. We don't really need to have them in the same directory as the semantic function, but I like doing that because it makes it easier to find them. We import them using the `import_skill` method.\n",
    "\n",
    "The function below classifies an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugins.image_classifier_plugin.image_classifier import ImageClassifierPlugin\n",
    "image_classifier = ImageClassifierPlugin()\n",
    "classify_plugin = kernel.import_skill(image_classifier, skill_name=\"classify_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a silly list of URLs to test the image classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.pixabay.com/photo/2016/02/10/16/37/cat-1192026_1280.jpg\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/fuzzychair.png\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/fail-whale.webp\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/rat.jpeg\"\n",
    "\n",
    "# other pictures to try: http://fun-pictube.blogspot.com/2012/05/animal-pictures-zoo-animal-pictures.html\n",
    "url = \"http://3.bp.blogspot.com/-fZK39AQB37M/T6Z1104yXWI/AAAAAAAAGko/c3Sv77URwPk/s1600/animal+pictures+%25285%2529.jpg\"\n",
    "url = \"http://2.bp.blogspot.com/-tG6z7DOsHNc/T6Z1DuzXs9I/AAAAAAAAGfY/YTmFDxw0Qxg/s320/animal+pictures+%252812%2529.jpg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiger\n"
     ]
    }
   ],
   "source": [
    "response = classify_plugin[\"classify_image\"](url)\n",
    "\n",
    "# get only up to the first comma, if it exists\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling multiple functions in sequence\n",
    "\n",
    "The big advantage of using a Kernel is that you can call multiple functions in sequence, and pass the output of one function to the next one. This allows you to do complex workflows in a simple call. \n",
    "\n",
    "### Telling a joke about an image\n",
    "\n",
    "In the example below, we pass an image as a URL, and then call the `classify_image` native function and the `cross_the_road` semantic function in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tiger cross the road?\n",
      "\n",
      "Because it was the chicken's day off.\n"
     ]
    }
   ],
   "source": [
    "context = kernel.create_new_context()\n",
    "context[\"input\"] = url\n",
    "\n",
    "response = await kernel.run_async(\n",
    "    classify_plugin[\"classify_image\"],\n",
    "    jokes_plugin[\"cross_the_road_joke\"],\n",
    "    input_context=context\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner\n",
    "\n",
    "The planner allows you to create an ask in natural language. The Semantic Kernel will search the plugins for a list of functions that matches the ask, and then execute them in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = BasicPlanner()\n",
    "ask = f\"\"\"Write a cross the road joke after classifying the image with this url: {url}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input\": \"http://2.bp.blogspot.com/-tG6z7DOsHNc/T6Z1DuzXs9I/AAAAAAAAGfY/YTmFDxw0Qxg/s320/animal+pictures+%252812%2529.jpg\",\n",
      "    \"subtasks\": [\n",
      "        {\"function\": \"classify_image.classify_image\"},\n",
      "        {\"function\": \"jokes.cross_the_road_joke\"}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "plan = await planner.create_plan_async(ask, kernel)\n",
    "print(plan.generated_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tiger cross the road?\n",
      "\n",
      "Because it wanted to show the chicken that it was possible!\n"
     ]
    }
   ],
   "source": [
    "joke_from_image = await planner.execute_plan_async(plan, kernel)\n",
    "print(joke_from_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
