{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A whirlwind tour through the Microsoft Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Microsoft Semantic Kernel](https://github.com/microsoft/semantic-kernel/tree/main) is what Microsoft uses to develop its Copilots. It's an open source package for C#, Python and Java that makes it easier to integrate code with Large Language Models (LLMs). The following Python notebook gives a quick tour of the most important functions of the package.\n",
    "\n",
    "To get started, you need to install the `semantic-kernel` package. I also use the `python-dotenv` package to load environment variables from a `.env` file, but if you're running this locally and don't expect to upload your notebook to GitHub, you can put your keys directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "from semantic_kernel.planning.basic_planner import BasicPlanner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm running my code on Azure OpenAI service. Another option is to use OpenAI services [directly from OpenAI](https://platform.openai.com/). In both cases, you'll need an API key. For Azure, you'll also need an endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "use_azure = True\n",
    "\n",
    "if use_azure:\n",
    "    OPENAI_ENDPOINT = os.getenv(\"OPENAI_USEAST3_ENDPOINT\")\n",
    "else:\n",
    "    ORG_ID = os.getenv(\"OPENAI_OAI_PERSONAL_DIRECT_ORG\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_USEAST3_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Kernel\n",
    "\n",
    "A cool thing about Semantic Kernel is that it supports multiple models. This enables you to run simple workloads on cheaper models, and expensive workloads on expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x25e4435a050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Note: this was not tested with use_azure = False\n",
    "\n",
    "if use_azure:\n",
    "    gpt35 = AzureChatCompletion(deployment_name=\"gpt-35-turbo\", # yours may be different\n",
    "                                endpoint=OPENAI_ENDPOINT,\n",
    "                                api_key=OPENAI_API_KEY)\n",
    "\n",
    "    gpt4 = AzureChatCompletion(deployment_name=\"gpt-4\", # yours may be different\n",
    "                                endpoint=OPENAI_ENDPOINT,\n",
    "                                api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    gpt35 = OpenAIChatCompletion(deployment_name=\"gpt-35-turbo\", # yours may be different\n",
    "                                org_id=ORG_ID,\n",
    "                                api_key=OPENAI_API_KEY)\n",
    "\n",
    "    gpt4 = OpenAIChatCompletion(deployment_name=\"gpt-4\", # yours may be different\n",
    "                                org_id = ORG_ID,\n",
    "                                api_key=OPENAI_API_KEY)    \n",
    "\n",
    "\n",
    "kernel.add_chat_service(\"gpt35\", gpt35)\n",
    "kernel.add_chat_service(\"gpt4\", gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the chat service\n",
    "\n",
    "One of the simplest things you can do is simply execute a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes a nice place you've got here!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"knock, knock? Whoâ€™s there? {{$input}}. {{$input}} who?\"\"\"\n",
    "knock = kernel.create_semantic_function(prompt, temperature=0.8)\n",
    "response = knock(\"Dishes\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic functions\n",
    "\n",
    "A **semantic function** is a function that interacts with a large language model (LLM).\n",
    "\n",
    "Although you can define a semantic function with a `dict` for the configuration and a `string` for the prompt as we did above, in production, we usually prefer to separate the code from the prompts and configuration. \n",
    "\n",
    "### Plugins\n",
    "\n",
    "Collections of semantic functions are called **Plugins**. Plugins are simply folders that contain semantic functions. Each semantic function should be in a separate folder.\n",
    "\n",
    "Each semantic function is defined by two files : `skprompt.txt` that contains the prompt (including placeholders for parameters) and `config.json`, that contains the configuration, such as default *temperature*, default *service*, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross_the_road_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x25e443a2a50>,\n",
       " 'genie_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x25e443a3a10>,\n",
       " 'knock_knock_joke': <semantic_kernel.orchestration.sk_function.SKFunction at 0x25e443b9b50>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_plugin = kernel.import_semantic_skill_from_directory(\"plugins\", \"jokes\")\n",
    "jokes_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a function from a plugin\n",
    "\n",
    "Once you loaded the functions into the Kernel, you can load them into a varaible or simply accessing them directly from the plugin object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knock, knock?\n",
      "Who's there?\n",
      "Dishes.\n",
      "Dishes who?\n",
      "Dishes a great joke, don't you think?\n"
     ]
    }
   ],
   "source": [
    "response = jokes_plugin[\"knock_knock_joke\"](\"dishes\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the function into a function variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the atom cross the road?\n",
      "\n",
      "Because it wanted to get away from the nuclear reaction.\n"
     ]
    }
   ],
   "source": [
    "cross_the_road = jokes_plugin[\"cross_the_road_joke\"]\n",
    "response = cross_the_road(\"atom\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `genie_joke` function has multiple parameters. To pass them, you have to create an object of the `ContextVariables` class, and pass it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three men were on a deserted island and found a genie. The genie gave each one wish. \n",
      "\n",
      "The first said he wished to go home and instantly turned into a homing pigeon.\n",
      "\n",
      "The second said he wished to go home and transformed into a teleporting Uber driver.\n",
      "\n",
      "The third person's wish was to have unlimited pizza deliveries, forgetting he was still on a deserted island.\n"
     ]
    }
   ],
   "source": [
    "context_variables = sk.ContextVariables()\n",
    "context_variables[\"firstWish\"] = \"go home\"\n",
    "context_variables[\"secondWish\"] = \"go home\"\n",
    "\n",
    "response = jokes_plugin[\"genie_joke\"](variables=context_variables)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native functions\n",
    "\n",
    "Native functions are pure Python code. We don't really need to have them in the same directory as the semantic function, but I like doing that because it makes it easier to find them. We import them using the `import_skill` method.\n",
    "\n",
    "The function below classifies an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugins.image_classifier_plugin.image_classifier import ImageClassifierPlugin\n",
    "image_classifier = ImageClassifierPlugin()\n",
    "classify_plugin = kernel.import_skill(image_classifier, skill_name=\"classify_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a silly list of URLs to test the image classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.pixabay.com/photo/2016/02/10/16/37/cat-1192026_1280.jpg\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/fuzzychair.png\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/fail-whale.webp\"\n",
    "url = \"https://mpsocial.blob.core.windows.net/blog-images/rat.jpeg\"\n",
    "\n",
    "# other pictures to try: http://fun-pictube.blogspot.com/2012/05/animal-pictures-zoo-animal-pictures.html\n",
    "url = \"http://3.bp.blogspot.com/-fZK39AQB37M/T6Z1104yXWI/AAAAAAAAGko/c3Sv77URwPk/s1600/animal+pictures+%25285%2529.jpg\"\n",
    "url = \"http://2.bp.blogspot.com/-tG6z7DOsHNc/T6Z1DuzXs9I/AAAAAAAAGfY/YTmFDxw0Qxg/s320/animal+pictures+%252812%2529.jpg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiger\n"
     ]
    }
   ],
   "source": [
    "response = classify_plugin[\"classify_image\"](url)\n",
    "\n",
    "# get only up to the first comma, if it exists\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling multiple functions in sequence\n",
    "\n",
    "The big advantage of using a Kernel is that you can call multiple functions in sequence, and pass the output of one function to the next one. This allows you to do complex workflows in a simple call. \n",
    "\n",
    "### Telling a joke about an image\n",
    "\n",
    "In the example below, we pass an image as a URL, and then call the `classify_image` native function and the `cross_the_road` semantic function in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tiger cross the road?\n",
      "\n",
      "Because it wanted to show the chicken that it's not the only one who can do it.\n"
     ]
    }
   ],
   "source": [
    "context = kernel.create_new_context()\n",
    "context[\"input\"] = url\n",
    "\n",
    "response = await kernel.run_async(\n",
    "    classify_plugin[\"classify_image\"],\n",
    "    jokes_plugin[\"cross_the_road_joke\"],\n",
    "    input_context=context\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner\n",
    "\n",
    "The planner allows you to create an ask in natural language. The Semantic Kernel will search the plugins for a list of functions that matches the ask, and then execute them in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = BasicPlanner()\n",
    "ask = f\"\"\"Write a cross the road joke after classifying the image with this url: {url}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input\": \"http://2.bp.blogspot.com/-tG6z7DOsHNc/T6Z1DuzXs9I/AAAAAAAAGfY/YTmFDxw0Qxg/s320/animal+pictures+%252812%2529.jpg\",\n",
      "    \"subtasks\": [\n",
      "        {\"function\": \"classify_image.classify_image\"},\n",
      "        {\"function\": \"jokes.cross_the_road_joke\"}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "plan = await planner.create_plan_async(ask, kernel)\n",
    "print(plan.generated_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tiger cross the road? \n",
      "\n",
      "Because it wanted to show the chicken that it's not the only one who can do it.\n"
     ]
    }
   ],
   "source": [
    "joke_from_image = await planner.execute_plan_async(plan, kernel)\n",
    "print(joke_from_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
